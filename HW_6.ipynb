{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ead699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cbf32f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29720\n",
       "1     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./Twitter Sentiment Analysis/train.csv\")\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9e8657",
   "metadata": {},
   "source": [
    "большой дисбаласн классов, подготовим 3 выборки, \n",
    "- X_train_samp - продублируем класс1 для сбалансированности, и на этой выборке будем производить обучение сети\n",
    "- X_train - выборку оставим без изменения что бы при обучении считать метрику на train\n",
    "- X_val - валидационная выборка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1160b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(df_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18bf02c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25252\n",
       "1     1915\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2c0efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = X_train.loc[X_train['label']==0]\n",
    "class1 = X_train.loc[X_train['label']==1]\n",
    "X_train_samp = class0.append(pd.DataFrame(np.repeat(class1.values, 13, axis=0), columns=class1.columns), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf2ed391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25252\n",
       "1    24895\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_samp = X_train_samp['label'].astype(np.int32)\n",
    "X_train_samp = X_train_samp.drop(columns=['label'])\n",
    "y_train_samp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d32889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25252\n",
       "1     1915\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = X_train['label'].astype(np.int32)\n",
    "X_train = X_train.drop(columns=['label'])\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ddd43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4468\n",
       "1     327\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = X_val['label']\n",
    "X_val = X_val.drop(columns=['label'])\n",
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799f0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c915032b",
   "metadata": {},
   "source": [
    "## Предпроцессинг текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53309d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "max_len = 20\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07f676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"en\"))\n",
    "puncts = set(punctuation)\n",
    "\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in puncts)\n",
    "    txt = txt.lower()\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d7364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50147/50147 [00:10<00:00, 4577.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 27167/27167 [00:05<00:00, 4801.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 4795/4795 [00:01<00:00, 4740.55it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "X_train_samp['tweet'] = X_train_samp['tweet'].progress_apply(preprocess_text)\n",
    "X_train['tweet'] = X_train['tweet'].progress_apply(preprocess_text)\n",
    "X_val['tweet'] = X_val['tweet'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9570968",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = \" \".join(X_train[\"tweet\"])\n",
    "train_corpus = train_corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12154f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\spvag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "tokens = word_tokenize(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1bef041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "dist = FreqDist(tokens_filtered)\n",
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]  # вычитание 1 для padding\n",
    "len(tokens_filtered_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90de6ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}\n",
    "# vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca2f1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "\n",
    "    padding = [0] * (maxlen-len(result))\n",
    "    return result[-maxlen:] + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "910cd69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in X_train[\"tweet\"]])\n",
    "x_valid = np.asarray([text_to_sequence(text, max_len) for text in X_val[\"tweet\"]])\n",
    "x_train_samp = np.asarray([text_to_sequence(text, max_len) for text in X_train_samp[\"tweet\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579555fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "985715c2",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc5eb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=2000, embedding_dim=256, out_channel=128, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_1 = nn.Conv1d(embedding_dim, out_channel, kernel_size=2)#2\n",
    "        self.conv_2 = nn.Conv1d(out_channel, out_channel*2, kernel_size=3)#3\n",
    "        self.conv_3 = nn.Conv1d(out_channel*2, out_channel*4, kernel_size=3)#3        \n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.linear_1 = nn.Linear(out_channel*4, out_channel*8)\n",
    "        self.linear_2 = nn.Linear(out_channel*8, out_channel*4)        \n",
    "        self.linear_3 = nn.Linear(out_channel*4, num_classes)\n",
    "        self.dp = nn.Dropout(0.65)\n",
    "        self.bn_1 = nn.BatchNorm1d(out_channel)\n",
    "        self.bn_2 = nn.BatchNorm1d(out_channel*2)        \n",
    "        self.bn_3 = nn.BatchNorm1d(out_channel*4)        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):        \n",
    "        output = self.embedding(x) # B, L, E      \n",
    "        output = output.permute(0, 2, 1)\n",
    "        output = self.conv_1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.bn_1(output)  \n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv_2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.bn_2(output)        \n",
    "        output = self.pool(output)\n",
    "       \n",
    "        output = self.conv_3(output)\n",
    "        output = self.relu(output)  \n",
    "        output = self.bn_3(output)      \n",
    "        output = torch.max(output, axis=2).values\n",
    "        output = self.linear_1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dp(output)\n",
    "        output = self.linear_2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dp(output)        \n",
    "        output = self.linear_3(output)        \n",
    "        output = F.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b373242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).long()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f0856ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 542\n",
    "print_batch_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b33254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataWrapper(x_train, y_train.values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = DataWrapper(x_valid, y_val.values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "train_dataset_samp = DataWrapper(x_train_samp, y_train_samp.values)\n",
    "train_loader_samp = DataLoader(train_dataset_samp, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dcd6ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(vocab_size=max_words)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e939f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(2000, 256)\n",
      "  (conv_1): Conv1d(256, 128, kernel_size=(2,), stride=(1,))\n",
      "  (conv_2): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
      "  (conv_3): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (linear_1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (linear_2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (linear_3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dp): Dropout(p=0.65, inplace=False)\n",
      "  (bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn_3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Parameters: 2122369\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(\"Parameters:\", sum([param.nelement() for param in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff6ac230",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87b8f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(model, loader):\n",
    "    model.eval()\n",
    "    running_total = 0.0\n",
    "    tp = 0.0\n",
    "    tp_fp = 0.0\n",
    "    tp_fn = 0.0\n",
    "    running_right = 0.0\n",
    "    for j, data in enumerate(loader):\n",
    "        labels = data[1].to(device)\n",
    "        outputs = model(data[0].to(device))\n",
    "        running_total += len(data[1])\n",
    "        pred_labels = torch.squeeze((outputs > th).int())\n",
    "        tp += (labels*pred_labels).sum()\n",
    "        tp_fp += pred_labels.sum()\n",
    "        tp_fn += labels.sum()\n",
    "        running_right += (labels == pred_labels).sum()\n",
    "    precision = tp/tp_fp\n",
    "    recall = tp/tp_fn\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    accuracy = running_right/running_total \n",
    "    model.train()    \n",
    "    return precision, recall, f1_score, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "670f7491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]. Step [93/93].  Loss: 0.688.  Train f1_score: 0.213.  Test f1_score: 0.210.\n",
      "Epoch [2/20]. Step [93/93].  Loss: 0.672.  Train f1_score: 0.246.  Test f1_score: 0.221.\n",
      "Epoch [3/20]. Step [93/93].  Loss: 0.638.  Train f1_score: 0.264.  Test f1_score: 0.240.\n",
      "Epoch [4/20]. Step [93/93].  Loss: 0.595.  Train f1_score: 0.285.  Test f1_score: 0.253.\n",
      "Epoch [5/20]. Step [93/93].  Loss: 0.523.  Train f1_score: 0.317.  Test f1_score: 0.285.\n",
      "Epoch [6/20]. Step [93/93].  Loss: 0.474.  Train f1_score: 0.358.  Test f1_score: 0.302.\n",
      "Epoch [7/20]. Step [93/93].  Loss: 0.395.  Train f1_score: 0.414.  Test f1_score: 0.335.\n",
      "Epoch [8/20]. Step [93/93].  Loss: 0.367.  Train f1_score: 0.441.  Test f1_score: 0.343.\n",
      "Epoch [9/20]. Step [93/93].  Loss: 0.299.  Train f1_score: 0.486.  Test f1_score: 0.355.\n",
      "Epoch [10/20]. Step [93/93].  Loss: 0.225.  Train f1_score: 0.564.  Test f1_score: 0.381.\n",
      "Epoch [11/20]. Step [93/93].  Loss: 0.202.  Train f1_score: 0.614.  Test f1_score: 0.398.\n",
      "Epoch [12/20]. Step [93/93].  Loss: 0.209.  Train f1_score: 0.635.  Test f1_score: 0.399.\n",
      "Epoch [13/20]. Step [93/93].  Loss: 0.150.  Train f1_score: 0.687.  Test f1_score: 0.399.\n",
      "Epoch [14/20]. Step [93/93].  Loss: 0.121.  Train f1_score: 0.752.  Test f1_score: 0.415.\n",
      "Epoch [15/20]. Step [93/93].  Loss: 0.087.  Train f1_score: 0.784.  Test f1_score: 0.424.\n",
      "Epoch [16/20]. Step [93/93].  Loss: 0.080.  Train f1_score: 0.822.  Test f1_score: 0.419.\n",
      "Epoch [17/20]. Step [93/93].  Loss: 0.108.  Train f1_score: 0.845.  Test f1_score: 0.419.\n",
      "Epoch [18/20]. Step [93/93].  Loss: 0.044.  Train f1_score: 0.873.  Test f1_score: 0.433.\n",
      "Epoch [19/20]. Step [93/93].  Loss: 0.035.  Train f1_score: 0.919.  Test f1_score: 0.456.\n",
      "Epoch [20/20]. Step [93/93].  Loss: 0.073.  Train f1_score: 0.906.  Test f1_score: 0.427.\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.train()\n",
    "th = 0.5\n",
    "\n",
    "train_history = []\n",
    "test_history = []\n",
    "\n",
    "for epoch in range(epochs):  \n",
    "    running_items, running_right = 0.0, 0.0\n",
    "    for i, data in enumerate(train_loader_samp, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "    precision_val, rerall_val, f1_score_val, acc_val = metrics(model, val_loader)\n",
    "    precision_train, rerall_train, f1_score_train, acc_train = metrics(model, train_loader)    \n",
    "    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
    "            f'Step [{i + 1}/{len(train_loader_samp)}].  ' \\\n",
    "            f'Loss: {loss:.3f}.  ' \\\n",
    "            f'Train f1_score: {f1_score_train:.3f}.  ' \\\n",
    "            f'Test f1_score: {f1_score_val:.3f}.'         \n",
    "         )\n",
    "    train_history.append([precision_train, rerall_train, f1_score_train, acc_train])\n",
    "    test_history.append([precision_val, rerall_val, f1_score_val, acc_val])    \n",
    "        \n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a83a76",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "\n",
    "В данных наблюдался большой дисбаланс классов. В силу того что в задании не были указаны конкретные метрики, в качестве оценки обучения сети я выбрал метрику f1, \n",
    "Accuracy не удачная метрика в данном случае.\n",
    "\n",
    "\n",
    "Для повышения метрики сети можно:\n",
    "-\tвыбрать более сложный алгоритм предобработки текста. \n",
    "-\tувеличить объём обучающих данных. Количество «твитов» 1-го класса слишком мало, поэтому сеть при обучении очень быстро переходит в режим переобучения, даже при больших значениях дропаута. \n",
    "\n",
    " Так же думаю, что увеличение количества слоёв или параметров сети не окажет значительного влияния на улучшение её характеристик. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e84434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
